#!/bin/bash
# Suffix (e.g. '<your name>') to facilitate running concurrent pipelines in the same Google Cloud project. Change if working in a team to avoid overwriting resources during development 
RESOURCE_SUFFIX=default
USER=${RESOURCE_SUFFIX}

# Default base image for components.
# Source: https://cloud.google.com/deep-learning-vm/docs/images
BASE_IMAGE=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-14.py310:latest

# GCP Project ID:
VERTEX_PROJECT_ID=gsd-ai-mx-mlops-demos
# Location:
VERTEX_LOCATION=us-central1
LOCATION=${VERTEX_LOCATION}
ZONE=${VERTEX_LOCATION}-c

# Artifact Registry Repository:
AR_BASE_NAME=vertex-ai
AR_PYTHON=${VERTEX_LOCATION}-python.pkg.dev/${VERTEX_PROJECT_ID}/${AR_BASE_NAME}-packages
AR_PIPELINES_REPO=${VERTEX_LOCATION}-python.pkg.dev/${VERTEX_PROJECT_ID}/${AR_BASE_NAME}-pipelines
AR_IMAGES_REPO=${VERTEX_LOCATION}-python.pkg.dev/${VERTEX_PROJECT_ID}/${AR_BASE_NAME}-container-images
CONTAINER_IMAGE_REGISTRY=${AR_IMAGES_REPO}
STAGING_IMGS_BUCKET=gs://${VERTEX_PROJECT_ID}-staging

BQ_LOCATION=US

# Default SA to run pipelines:
SA_NAME=vertex-ai-pipelines-sa
VERTEX_SA_EMAIL=${SA_NAME}@${VERTEX_PROJECT_ID}.iam.gserviceaccount.com
# Bucket to save pipeline files:
VERTEX_PIPELINE_ROOT=gs://${VERTEX_PROJECT_ID}-pl-root

# PUB/SUB AND CLOUD FUNCTION:
# Pubsub Topic to run pipelines on a schedule or by event triggers:
# Uri: https://run-vertex-pipelines-nmq5muaadq-uc.a.run.app
PUBSUB_TOPIC=vertex-pipelines-topic
# Cloud function info:
CLOUD_FUNCTION=run-vertex-pipelines
SOURCE=cloudfunction/src/
ENTRY_POINT=cf_handler
RUN_TIME=python39

# (Optional) Network config:
VERTEX_NETWORK=
VERTEX_CMEK_IDENTIFIER=
