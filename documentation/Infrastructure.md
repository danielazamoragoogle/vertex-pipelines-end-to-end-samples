<!-- 
Copyright 2023 Google LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
 -->
# Infrastructure Set Up

## Cloud Resources (Desired State)

This section outlines the cloud infrastructure required for our project, which we provision and manage using Terraform (steps in the following section). The scripts provided in the Golden Path help us enable and configure the following Google Cloud Platform (GCP) services and resources:

* **Vertex AI \[service\]:** Google Cloud's unified machine learning platform, for developing, executing, and managing our machine learning pipelines. We create the following resources:  
  * \`default\_store\`: **Vertex AI Metadata Store \[resource\]** to meticulously track metadata associated with our pipelines, including details about executions, lineage information, and artifacts.  
  * \`workbench-\[USER\]\`: A **Vertex AI Workbench** **Instance  \[resource\]** for each of the project users, so that they can easily start developing.  
* **Artifact Registry \[service\]:** Artifact Registry serves as a centralized repository to store and manage artifacts vital to our machine learning workflows. We create the following **AR Repositories \[resource\]**:  
  * \`vertex-ai-container-images\` (Format: DOCKER): To store Docker container images, specifically for housing custom Vertex AI pipeline components.  
  * \`vertex-ai-pipelines\` (Format: KFP): To store our compiled Kubeflow Pipelines (KFP) pipeline definitions.  
  * \`vertex-ai-packages\` (Format: PYTHON): Designed specifically for managing Python Package resources, which are used within our pipelines.  
* **Cloud Storage \[service\]:** We enable the service and provision a **Cloud Storage bucket \[resource\]** named \`\[project-id\]-vertex-ai-pipeline-artifacts\` as the central storage location for artifacts generated by our Vertex AI pipelines. This bucket ensures secure and scalable storage for pipeline outputs and intermediate files.  
* **BigQuery \[service\]:** Highly scalable and serverless data warehouse, for data processing and analysis tasks within our pipelines.  
* **Cloud Build \[service\]:** Fully managed CI/CD (Continuous Integration/Continuous Delivery) platform, to automate the building and deployment of our pipeline components (or any used image).  
* **Cloud Monitoring \[service\]:** Cloud Monitoring provides visibility into the performance of the processes within pipelines. We configure it to collect metrics and logs, allowing us to proactively identify and address potential issues, optimize performance, and ensure the reliability of our machine learning workflows.  
* **IAM \[service\]:** To maintain robust security and control access to our resources. We also create a **Service Account \[resource\]** dedicated to the project \`vertex-ai-pipelines-sa\`, and define Identity and Access Management (IAM) roles and policies, both for the service account and a group of users in the project, here's a breakdown of the roles:  
  * Permissions for service account (\`vertex-ai-pipelines-sa\`):  
    * roles/aiplatform.user: Grants essential permissions for interacting with Vertex AI resources.  
    * roles/artifactregistry.reader: Allows reading artifacts from Artifact Registry repositories.  
    * roles/bigquery.dataEditor, roles/bigquery.jobUser, roles/bigquery.user: Provides comprehensive access to BigQuery for data manipulation, job execution, and data analysis.  
    * roles/logging.logWriter: Enables the service account to write logs, which is crucial for monitoring pipeline executions, debugging, and troubleshooting.  
    * roles/monitoring.metricWriter: Grants permission to write custom metrics. This is important for monitoring pipeline health and performance, allowing us to create dashboards and alerts based on these metrics.  
    * roles/secretmanager.secretAccessor: Provides read-access to secrets stored in Secret Manager, ensuring our pipelines can securely access confidential information.  
    * roles/storage.admin, roles/storage.objectUser: Grants comprehensive access to Cloud Storage (more limited with objectUser) enabling the service account to manage and interact with pipeline artifacts stored in the designated Cloud Storage bucket.  
  * Permissions for users (emails, usernames, and permissions defined in the variables.tf file):   
    * roles/artifactregistry.admin: Grants full administrative control over Artifact Registry.  
    * roles/bigquery.dataEditor, roles/bigquery.jobUser, roles/bigquery.user: Mirrors the BigQuery permissions assigned to the pipeline service account.  
    * roles/cloudbuild.editor: Enables users to work with Cloud Build for building and deploying containers (to AR, for example).  
    * roles/logging.viewer, roles/logging.logWriter: Provides permissions for viewing and writing logs, essential for monitoring and debugging.  
    * roles/notebooks.admin: Grants administrative control over Vertex AI Workbench notebooks.  
    * roles/secretmanager.secretAccessor, roles/secretmanager.secretVersionAdder: Grants access to secrets in Secret Manager, allowing users to work with sensitive information securely.  
    * roles/serviceusage.serviceUsageConsumer: Enables users to view service usage information.  
    * roles/iam.serviceAccountUser: You might need to grant this role to specific users (with caution) so they can manage or interact with service accounts or potentially run code that needs to impersonate a service account (recommended to add fine-grained access to specific service accounts, if possible).  
    * roles/storage.admin, roles/storage.objectUser: Provides access to Cloud Storage, similar to the pipeline service account.  
    * roles/aiplatform.admin: Grants comprehensive administrative control over Vertex AI.  
    * roles/viewer: Provides read-only access across most GCP services, ideal for auditing and monitoring.

The Terraform configuration files define this desired state of our infrastructure (main.tf, variables.tf, iam.tf).

## Execution (Terraform Scripts)

The following instructions guide a user (who must be a Project Owner) to execute the Terraform scripts to deploy the required cloud infrastructure.

1. Activate a Cloud Shell terminal (fully authenticated interactive shell environment with \`gcloud\` and \`terraform\` pre-installed). You may me prompted instructions to set up your account and project.  

    ![shell](https://screenshot.googleplex.com/3Kysmg9JFj2UM4w.png)  

2. Validate the terraform version and perform and upgrade if needed:  
   

    ```
    terraform --version
    ```

    ```
    wget https://releases.hashicorp.com/terraform/1.9.6/terraform_1.9.6_linux_amd64.zip 
    unzip -q terraform_1.9.6_linux_amd64.zip
    sudo mv terraform /usr/bin
    ```

3. Create a repository from the template.  
4. Clone your template to the Cloud Shell environment:

    ```
    git clone [repository_url]
    ```

5. Rename and edit your environment variables, you can **open a VSCode-like editor to better handle your file changes**:  
   ![EDITOR](https://screenshot.googleplex.com/4hXXjCXiTq3jhiu.png)
   Rename the \`env.sh.example\` file to \`env.sh\`and edit the VERTEX\_PROJECT\_ID, VERTEX\_LOCATION, VERTEX\_ZONE, and RESOURCE\_SUFFIX.  
   ![env](https://screenshot.googleplex.com/644eNMHz9HP67xG.png)

6. Under \`terraform\>modules\>vertex\_deployment\>variables.tf\` add the emails and usernames of the people that will be using this template, this will provide them with the IAM permissions mentioned above.  
7. **Return to the terminal.**  
   ![terminal](https://screenshot.googleplex.com/BmWW6pY9VpBkHTu.png) 
8. Create a GCS bucket to store your terraform state (on the root directory):

    ```
    source env.sh && gsutil mb -l $VERTEX_LOCATION -p $VERTEX_PROJECT_ID gs://$VERTEX_PROJECT_ID-tfstate
    ```

9. Review the deployment plan to avoid any conflicts. This step shows the resources that will be created, modified, or destroyed, allowing for careful review before applying any changes (Make \`plan\` target (i) initializes Terraform using the GCS state bucket and then (ii) uses a Terraform plan):

    ```
    make plan
    ```

10. Apply the configuration (this will provision the infrastructure previously mentioned in the section ‚ÄúCloud Resources (Desired State)‚Äù and checked in the previous step): 

    ```
    make deploy
    ```

The Google Cloud Setup is ready üöÄ.